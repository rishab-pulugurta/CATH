{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#installs for mmseq\n",
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JoPoC9oFHSIR",
        "outputId": "635bd8c0-f4d4-4026-b1fb-514728c5bb4a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✨🍰✨ Everything looks OK!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#mmseq\n",
        "!conda install -c conda-forge -c bioconda mmseqs2"
      ],
      "metadata": {
        "collapsed": true,
        "id": "m-boERzRHWAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#installs\n",
        "!pip install biopython\n",
        "!pip install py3dmol\n",
        "!pip install transformers\n",
        "!pip install fair-esm\n",
        "!pip install scikit-learn\n",
        "!pip install torch-geometric"
      ],
      "metadata": {
        "collapsed": true,
        "id": "PpxliSLlHc5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#imports\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "import torch\n",
        "import esm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import io\n",
        "from google.colab import drive\n",
        "from transformers import EsmModel, EsmTokenizer, EsmConfig, AutoTokenizer\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from google.colab import drive\n",
        "from Bio import SeqIO\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "from Bio.PDB import PDBParser\n",
        "from torch_geometric.data import Data, DataLoader\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from torch_geometric.nn import GATConv, global_max_pool\n",
        "from torch.optim import Adam\n",
        "import requests\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "\n",
        "#connect to drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "7fW664puhpN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load in train df\n",
        "df = pd.read_csv('/content/drive/MyDrive/Challenge (Rishab)/ml_hands_on_challenge/final_data.csv')"
      ],
      "metadata": {
        "id": "392tX-GoITtv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "9xkUB8OXeDBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Initial Dataset Pre-Processing**"
      ],
      "metadata": {
        "id": "3-u33n2DhoOh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "architecture_names = {\n",
        "    (1, 10): \"Mainly Alpha: Orthogonal Bundle\",\n",
        "    (1, 20): \"Mainly Alpha: Up-down Bundle\",\n",
        "    (2, 30): \"Mainly Beta: Roll\",\n",
        "    (2, 40): \"Mainly Beta: Beta Barrel\",\n",
        "    (2, 60): \"Mainly Beta: Sandwich\",\n",
        "    (3, 10): \"Alpha Beta: Roll\",\n",
        "    (3, 20): \"Alpha Beta: Alpha-Beta Barrel\",\n",
        "    (3, 30): \"Alpha Beta: 2-Layer Sandwich\",\n",
        "    (3, 40): \"Alpha Beta: 3-Layer(aba) Sandwich\",\n",
        "    (3, 90): \"Alpha Beta: Alpha-Beta Complex\"\n",
        "}"
      ],
      "metadata": {
        "id": "CClIXYFpiZ-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_architecture_name(row):\n",
        "    key = (row['class'], row['architecture'])\n",
        "    return architecture_names.get(key, \"Unknown\")\n",
        "\n",
        "#add names\n",
        "df['architecture_domain'] = df.apply(get_architecture_name, axis=1)"
      ],
      "metadata": {
        "id": "h_O0DeVGiI6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Clustering For Homology**"
      ],
      "metadata": {
        "id": "w9-7i1L-cPbw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#check for invalid amino acids\n",
        "\n",
        "#define set {} of valid AAs\n",
        "valid_AAs = {'A', 'R', 'N', 'D', 'C', 'E', 'Q', 'G', 'H', 'I', 'L', 'K', 'M', 'F', 'P', 'S', 'T', 'W', 'Y', 'V'}\n",
        "\n",
        "#define method to check a sequence for invalid characters\n",
        "def contains_invalid_char(seq):\n",
        "  unique_chars = set(seq) # set of all characters in the protein; unique_chars = {A, C} for protein=\"AAACCC\"\n",
        "\n",
        "  if unique_chars.issubset(valid_AAs):  #unique_chars = {A,C}, and {A,C} is a subset of valid_AAs\n",
        "    return ''\n",
        "  else: # e.g. unique_chars = {A,X}. {A,X} is not a subset of valid_AAs because X is not in valid_AAs\n",
        "    return unique_chars.difference(valid_AAs) # e.g. {A,X} - valid_AAs = {X}"
      ],
      "metadata": {
        "id": "9bs8jHlGIZ4J"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#apply our method contains_invalid_char to the sequence column\n",
        "df['invalid_chars'] = df['sequences'].apply(contains_invalid_char)\n",
        "#display rows where there's an invalid character\n",
        "df[df['invalid_chars'].str.len()>0].sort_values(by='sequences')"
      ],
      "metadata": {
        "id": "jVbkWZZEIa2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#remove invalid rows if necessary\n",
        "df = df[df['invalid_chars'].str.len()==0].reset_index(drop=True).drop(columns=['invalid_chars'])"
      ],
      "metadata": {
        "id": "mCt0cQqsIkgG"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#reset df\n",
        "df.reset_index(drop=True, inplace=True)"
      ],
      "metadata": {
        "id": "r1FWILrDInyF"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#add an ID for clustering\n",
        "df['id'] = [f'seq{i}' for i in range(len(df))]"
      ],
      "metadata": {
        "id": "IwMxyLczItpu"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#write the fasta file\n",
        "with open('sequences.fasta', 'w') as f:\n",
        "  for i in range(len(df)):\n",
        "    id = df.loc[i,'id']\n",
        "    seq = df.loc[i,'sequences']\n",
        "    f.write(f'>{id}\\n{seq}\\n')"
      ],
      "metadata": {
        "id": "fKVCuVr7IxL4"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#clustering with mmseqs to solve homology issue\n",
        "!mmseqs easy-cluster sequences.fasta clusterRes mmseqs_results --min-seq-id 0.2 -c 0.3 --cov-mode 0"
      ],
      "metadata": {
        "collapsed": true,
        "id": "4LrNrnK2I3pM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get clusters\n",
        "fasta_sequences = SeqIO.parse(open('clusterRes_rep_seq.fasta'),'fasta')\n",
        "id_list = []\n",
        "seq_list = []\n",
        "\n",
        "for fasta in fasta_sequences:\n",
        "  id, sequence = fasta.id, str(fasta.seq)\n",
        "\n",
        "  id_list.append(id)\n",
        "  seq_list.append(sequence)\n",
        "\n",
        "cluster_reps = pd.DataFrame(\n",
        "    data = {\n",
        "        'representative id': id_list,\n",
        "        'sequence': seq_list\n",
        "    }\n",
        ")\n",
        "\n",
        "print('Total clusters: {}'.format(len(cluster_reps)))\n",
        "cluster_reps.head()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ytdYwtgNJAhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#make sure all sequences are clusterred\n",
        "clusters = pd.read_csv(f'clusterRes_cluster.tsv',sep='\\t',header=None)\n",
        "print('Total cluster members: {}'.format(len(clusters)))\n",
        "\n",
        "\n",
        "print('All sequences were clustered: {}'.format(len(clusters)==len(df)))\n",
        "\n",
        "clusters = clusters.rename(columns={\n",
        "    0: 'representative id',\n",
        "    1: 'member id'\n",
        "})\n",
        "clusters.head()"
      ],
      "metadata": {
        "id": "x02KaXTKJy9G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#make clusters df\n",
        "clusters = pd.merge(clusters,\n",
        "                    df.rename(columns={'id': 'member id',\n",
        "                                            'Length':'length',\n",
        "                                            'sequences':'sequence'}),\n",
        "                    on='member id',how='left')\n",
        "clusters"
      ],
      "metadata": {
        "collapsed": true,
        "id": "xRPEP4tmJ2kh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#grouping by representative id creates one row per cluster\n",
        "#counting the member ID column for each cluster gives you the size of that cluster\n",
        "sizedict = pd.DataFrame(clusters.groupby('representative id').count()['member id']).to_dict()['member id']\n",
        "\n",
        "#include the size of each cluster\n",
        "clusters['cluster_size'] = clusters['representative id'].apply(lambda x: sizedict[x])\n",
        "#sort by cluster size\n",
        "clusters = clusters.sort_values(by='cluster_size',ascending=False).reset_index(drop=True)\n",
        "\n",
        "clusters"
      ],
      "metadata": {
        "collapsed": true,
        "id": "7nVU0jKlKIRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#prepare data for the random cluster split\n",
        "X = list(clusters['representative id'])\n",
        "y = ['']*len(X) # there are no target values for clusters - this array will effectively be blank\n",
        "\n",
        "#fix the random state\n",
        "rs=78\n",
        "\n",
        "#perform the split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=rs) # 80-20 train-test\n",
        "\n",
        "#split the dataframe describing the clusters into train and test\n",
        "train_clusters = clusters[clusters['representative id'].isin(X_train)].sort_values(by=['cluster_size'],ascending=False).reset_index(drop=True)\n",
        "test_clusters = clusters[clusters['representative id'].isin(X_test)].sort_values(by=['cluster_size'],ascending=False).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "AyUyxN97KMAg"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get the train cluster representatives\n",
        "train_cluster_reps = list(train_clusters['representative id'])\n",
        "#use the clusters database to get the sequences of each protein in each cluster\n",
        "train_sequences = list(clusters.loc[clusters['representative id'].isin(train_cluster_reps)]['sequence'])\n",
        "#use the clusters database to get the flo values of each protein in each cluster\n",
        "train_targets = list(clusters.loc[clusters['representative id'].isin(train_cluster_reps)]['architecture'])\n",
        "print('train sequences: ', len(train_sequences))\n",
        "\n",
        "test_cluster_reps = list(test_clusters['representative id'])\n",
        "test_sequences = list(clusters.loc[clusters['representative id'].isin(test_cluster_reps)]['sequence'])\n",
        "test_targets = list(clusters.loc[clusters['representative id'].isin(test_cluster_reps)]['architecture'])\n",
        "print('test sequences: ', len(test_sequences))"
      ],
      "metadata": {
        "id": "CIjqU1LfKUTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Generating Structure Data**"
      ],
      "metadata": {
        "id": "V1h-ZGxScY4F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#unzip the PDB files\n",
        "!unzip '/content/drive/MyDrive/Challenge (Rishab)/ml_hands_on_challenge/pdb_share.zip' -d /content/pdb_files"
      ],
      "metadata": {
        "collapsed": true,
        "id": "F2hOY_PyChlx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pdb directory\n",
        "pdb_dir = '/content/pdb_files/pdb_share'\n",
        "\n",
        "#store structure data\n",
        "data_list = []\n",
        "\n",
        "parser = PDBParser(QUIET=True)\n",
        "\n",
        "pdb_files = os.listdir(pdb_dir)\n",
        "\n",
        "for pdb_file in tqdm(pdb_files, desc=\"Loading PDB files\"):\n",
        "    pdb_path = os.path.join(pdb_dir, pdb_file)\n",
        "    if os.path.isfile(pdb_path):\n",
        "        structure = parser.get_structure('protein', pdb_path)\n",
        "\n",
        "        atom_coords = []\n",
        "        atom_types = []\n",
        "        edge_index = []\n",
        "\n",
        "        for model in structure:\n",
        "            for chain in model:\n",
        "                for residue in chain:\n",
        "                    for atom in residue:\n",
        "                        atom_coords.append(atom.coord.tolist())\n",
        "                        atom_types.append(atom.element)\n",
        "\n",
        "        num_atoms = len(atom_coords)\n",
        "        edge_index = []\n",
        "\n",
        "        if num_atoms == 0:\n",
        "            continue\n",
        "\n",
        "        for i in range(num_atoms):\n",
        "            for j in range(i+1, num_atoms):\n",
        "                distance = np.linalg.norm(np.array(atom_coords[i]) - np.array(atom_coords[j]))\n",
        "                if distance < 5.0:  #using 5 Angstroms from literature\n",
        "                    edge_index.append([i, j])\n",
        "                    edge_index.append([j, i])\n",
        "\n",
        "        x = torch.tensor(atom_coords, dtype=torch.float)\n",
        "        edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
        "\n",
        "        identifier = os.path.splitext(pdb_file)[0]  #allows us to map back to cath_id\n",
        "\n",
        "        data = Data(x=x, edge_index=edge_index, pdb_id=identifier)\n",
        "\n",
        "        data_list.append(data)"
      ],
      "metadata": {
        "id": "Shs50ZibkC9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load in saved structure data\n",
        "data_list = torch.load('/content/drive/MyDrive/Challenge (Rishab)/ml_hands_on_challenge/structure_data.pt')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "AVqRTbuxRaVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#add structure data to df\n",
        "structural_data_dict = {data.pdb_id: data for data in data_list}\n",
        "\n",
        "df['structural_data'] = df['cath_id'].map(structural_data_dict)"
      ],
      "metadata": {
        "id": "W12giVOifzd0"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training GNN For Structure Embeddings**"
      ],
      "metadata": {
        "id": "uHYwUl8qdXb-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label_encoder = LabelEncoder()\n",
        "df['encoded_architecture'] = label_encoder.fit_transform(df['architecture'])"
      ],
      "metadata": {
        "id": "zOEZz-N4jTzX"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dataset setup\n",
        "\n",
        "class StructureDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.df = df\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        structure_data = self.df.iloc[idx]['structural_data']\n",
        "        architecture = self.df.iloc[idx]['encoded_architecture']\n",
        "        architecture = torch.tensor(architecture, dtype=torch.long)\n",
        "\n",
        "        return {\n",
        "            'Structure': structure_data,\n",
        "            'Architecture': architecture\n",
        "        }"
      ],
      "metadata": {
        "id": "EYGZQ3IoggIc"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model (use classifier to train, only take embeddings from model)\n",
        "class GATClassifier(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, num_heads=8, dropout_rate=0.3):\n",
        "        super(GATClassifier, self).__init__()\n",
        "        self.conv1 = GATConv(in_channels, hidden_channels // num_heads, heads=num_heads)\n",
        "        self.conv2 = GATConv(hidden_channels, hidden_channels // num_heads, heads=num_heads)\n",
        "        self.conv3 = GATConv(hidden_channels, hidden_channels // num_heads, heads=num_heads)\n",
        "\n",
        "        self.dropout = nn.Dropout(p=dropout_rate)\n",
        "\n",
        "        #fully connected layers\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(hidden_channels, 320),\n",
        "            nn.ReLU(),\n",
        "            self.dropout,\n",
        "            nn.Linear(320, 128),\n",
        "            nn.ReLU(),\n",
        "            self.dropout,\n",
        "            nn.Linear(128, out_channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, data):\n",
        "        #get graph data\n",
        "        structure_data = data['Structure']\n",
        "        x, edge_index, batch = structure_data.x, structure_data.edge_index, structure_data.batch\n",
        "\n",
        "        #add skip connections\n",
        "        x1 = F.relu(self.conv1(x, edge_index))\n",
        "        x2 = F.relu(self.conv2(x1, edge_index)) + x1\n",
        "        x3 = self.conv3(x2, edge_index) + x2\n",
        "\n",
        "        #max pooling\n",
        "        x = global_max_pool(x3, batch)\n",
        "\n",
        "        #pass through fc layers\n",
        "        out = self.fc_layers(x)\n",
        "\n",
        "        return out, x"
      ],
      "metadata": {
        "id": "cOt487Y1ha-M"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#initialize model, optimizer, loss function\n",
        "structure_model = GATClassifier(in_channels=3, hidden_channels=640, out_channels=10, num_heads=8).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "N63kZ_OSeAWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = StructureDataset(df)\n",
        "\n",
        "#split data (only split for embeddings)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
        "\n",
        "#dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "AjhPrmSDkXGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "best_accuracy = 0.0  #keep track of accuracy\n",
        "best_model_path = \"/content/drive/MyDrive/Challenge (Rishab)/ml_hands_on_challenge/structure_model.pt\"\n",
        "\n",
        "#train\n",
        "for epoch in range(num_epochs):\n",
        "    structure_model.train()\n",
        "    total_loss = 0\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}:\")\n",
        "\n",
        "    for data in tqdm(train_loader, desc=\"Training Batches\"):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        structure_data = data['Structure'].to(device)\n",
        "        architecture = data['Architecture'].view(-1).to(device)\n",
        "\n",
        "        out, _ = structure_model({'Structure': structure_data})\n",
        "\n",
        "        loss = criterion(out, architecture)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    average_loss = total_loss / len(train_loader)\n",
        "\n",
        "    #eval\n",
        "    structure_model.eval()\n",
        "    correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in tqdm(test_loader, desc=\"Testing Batches\")\n",
        "            structure_data = data['Structure'].to(device)\n",
        "            architecture = data['Architecture'].view(-1).to(device)\n",
        "\n",
        "            out, _ = structure_model({'Structure': structure_data})\n",
        "\n",
        "            pred = out.argmax(dim=1)\n",
        "            correct += (pred == architecture).sum().item()\n",
        "\n",
        "    accuracy = correct / len(test_loader.dataset)\n",
        "\n",
        "    #results\n",
        "    print(f\"Epoch {epoch + 1} - Loss: {average_loss:.4f}, Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    #save only best models\n",
        "    if accuracy > best_accuracy:\n",
        "        best_accuracy = accuracy\n",
        "        torch.save(structure_model.state_dict(), best_model_path)\n",
        "        print(f\"Best model saved with accuracy: {best_accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "_OlUInOElFFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get structure embeddings\n",
        "structure_model.eval()\n",
        "\n",
        "structure_model = structure_model.to(device)\n",
        "\n",
        "structure_embeddings = {}\n",
        "\n",
        "with torch.no_grad():\n",
        "    for idx, row in tqdm(df.iterrows()):\n",
        "        #get data\n",
        "        structure_data = row['structural_data']\n",
        "        sequence = row['sequences']\n",
        "\n",
        "        structure_data = structure_data.to(device)\n",
        "\n",
        "        input_data = {'Structure': structure_data}\n",
        "\n",
        "        #get embeddings\n",
        "        _, embedding = structure_model(input_data)\n",
        "\n",
        "        structure_embeddings[sequence] = embedding"
      ],
      "metadata": {
        "id": "yYyUHt6roELs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#save embeddings as pkl file\n",
        "with open('/content/drive/MyDrive/Challenge (Rishab)/ml_hands_on_challenge/structure_embeddings.pkl', 'wb') as file:\n",
        "  pickle.dump(structure_embeddings, file)"
      ],
      "metadata": {
        "id": "Q3u3jFxLvNNE"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Generate ESM Sequence Embeddings**"
      ],
      "metadata": {
        "id": "k-FwUV-LfkMA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load esm model\n",
        "model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
        "batch_converter = alphabet.get_batch_converter()\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "vXMb6RpN1pAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#run embeddings\n",
        "sequence_embeddings = {}\n",
        "\n",
        "for sequence in tqdm(df['sequences']):\n",
        "\n",
        "    #standard esm calculations\n",
        "    batch_labels, batch_strs, batch_tokens = batch_converter([(\"\", sequence)])\n",
        "    batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        results = esm_model(batch_tokens, repr_layers=[33], return_contacts=True)\n",
        "    token_representations = results[\"representations\"][33].cpu()\n",
        "    del batch_tokens\n",
        "\n",
        "    avg_embedding = token_representations[0, 1 : batch_lens[0] - 1].mean(0)\n",
        "    sequence_embeddings[sequence] = avg_embedding"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "QUL0-gsZz2z6",
        "outputId": "e41ee29d-3907-4221-a84d-0070594b41ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6263/6263 [1:05:24<00:00,  1.60it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#save embeddings as pkl file\n",
        "with open('/content/drive/MyDrive/Challenge (Rishab)/ml_hands_on_challenge/cath_proteins_embeddings.pkl', 'wb') as file:\n",
        "  pickle.dump(sequence_embeddings, file)"
      ],
      "metadata": {
        "id": "vxdQC4931vE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Handling Missing Amino Acids**"
      ],
      "metadata": {
        "id": "PBEmJO5wf-G4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#find indices of missing amino acids based on pdb\n",
        "def extract_residue_indices(pdb_file):\n",
        "    parser = PDBParser(QUIET=True)\n",
        "    structure = parser.get_structure('', pdb_file)\n",
        "    indices = set()\n",
        "    for model in structure:\n",
        "        for chain in model:\n",
        "            for residue in chain.get_residues():\n",
        "                indices.add(residue.id[1])\n",
        "    print(f\"Extracted indices from {pdb_file}: {sorted(indices)}\")\n",
        "    return indices\n",
        "\n",
        "#check for gaps based on cath indices and pdb residue indices\n",
        "def check_for_gaps(cath_indices, pdb_indices):\n",
        "    missing_indices = []\n",
        "    for start, end in cath_indices:\n",
        "        missing = [index for index in range(start, end + 1) if index not in pdb_indices]\n",
        "        if missing:\n",
        "            print(f\"Gap detected for range {start}-{end} with PDB indices: {sorted(pdb_indices)}\")\n",
        "            print(f\"Missing indices: {missing}\")\n",
        "            missing_indices.extend(missing)\n",
        "    return missing_indices\n",
        "\n",
        "#directory of pdbs\n",
        "pdb_dir = '/content/pdb_files/pdb_share'\n",
        "\n",
        "results = []\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    pdb_id = row['pdb_id']\n",
        "    cath_id = row['cath_id']\n",
        "\n",
        "    #find cath indices\n",
        "    cath_indices = row['cath_indices']\n",
        "\n",
        "    if pd.isna(cath_indices):\n",
        "        continue\n",
        "\n",
        "    if isinstance(cath_indices, str):\n",
        "        cath_indices = eval(cath_indices)\n",
        "    elif not isinstance(cath_indices, list):\n",
        "        raise ValueError(f\"Unsupported format for CATH indices at row {index}: {cath_indices}\")\n",
        "\n",
        "    pdb_file = os.path.join(pdb_dir, cath_id)\n",
        "    if os.path.exists(pdb_file):\n",
        "        print(f\"Processing PDB file: {pdb_file} for row {index}\")\n",
        "        pdb_indices = extract_residue_indices(pdb_file)\n",
        "        missing_indices = check_for_gaps(cath_indices, pdb_indices)\n",
        "        #add indices that are missing\n",
        "        results.append((pdb_id, cath_id, missing_indices))\n",
        "\n",
        "results_df = pd.DataFrame(results, columns=['pdb_id', 'cath_id', 'missing_indices'])"
      ],
      "metadata": {
        "collapsed": true,
        "id": "6QyG3ALYClTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#go through pdb id/chain/subunit and find original sequence from either uniprot or rscb\n",
        "\n",
        "def get_uniprot_id(pdb_id, polymer, chain_id=None):\n",
        "    polymer_url = f\"https://data.rcsb.org/rest/v1/core/polymer_entity/{pdb_id}/{polymer}\"\n",
        "    try:\n",
        "        polymer_response = requests.get(polymer_url)\n",
        "        polymer_response.raise_for_status()\n",
        "        polymer_data = polymer_response.json()\n",
        "        if chain_id:\n",
        "            if chain_id in polymer_data['rcsb_polymer_entity_container_identifiers']['auth_asym_ids']:\n",
        "                uniprot_id = polymer_data['rcsb_polymer_entity_container_identifiers']['reference_sequence_identifiers'][0]['database_accession']\n",
        "                return uniprot_id\n",
        "        else:\n",
        "            uniprot_id = polymer_data['rcsb_polymer_entity_container_identifiers']['reference_sequence_identifiers'][0]['database_accession']\n",
        "            return uniprot_id\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching UniProt ID for PDB ID {pdb_id}, polymer {polymer}: {e}\")\n",
        "        return None\n",
        "    except (KeyError, IndexError):\n",
        "        print(f\"Error extracting UniProt ID from response for PDB ID {pdb_id}, polymer {polymer}\")\n",
        "        return None\n",
        "\n",
        "def get_sequence_from_uniprot(uniprot_id):\n",
        "    base_url = f\"https://rest.uniprot.org/uniprotkb/{uniprot_id}.fasta\"\n",
        "    try:\n",
        "        response = requests.get(base_url)\n",
        "        response.raise_for_status()\n",
        "        fasta_data = response.text\n",
        "        sequence = ''.join(fasta_data.split('\\n')[1:])\n",
        "        return sequence\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching sequence for UniProt ID {uniprot_id}: {e}\")\n",
        "        return None\n",
        "\n",
        "def get_sequence_from_pdb(pdb_id, polymer):\n",
        "    polymer_url = f\"https://data.rcsb.org/rest/v1/core/polymer_entity/{pdb_id}/{polymer}\"\n",
        "    try:\n",
        "        response = requests.get(polymer_url)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "        sequence = data['entity_poly']['pdbx_seq_one_letter_code_can']\n",
        "        return sequence\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching sequence from PDB for PDB ID {pdb_id}, polymer {polymer}: {e}\")\n",
        "        return None\n",
        "    except KeyError:\n",
        "        print(f\"Error extracting sequence from PDB response for PDB ID {pdb_id}, polymer {polymer}\")\n",
        "        return None\n",
        "\n",
        "def get_sequence(pdb_id, identifier):\n",
        "    base_url = \"https://data.rcsb.org/rest/v1/core/entry\"\n",
        "    entry_url = f\"{base_url}/{pdb_id}\"\n",
        "    try:\n",
        "        response = requests.get(entry_url)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching entry details for PDB ID {pdb_id}: {e}\")\n",
        "        return None\n",
        "    except ValueError:\n",
        "        print(f\"Error decoding JSON response for PDB ID {pdb_id}\")\n",
        "        print(f\"Response content: {response.text}\")\n",
        "        return None\n",
        "\n",
        "    chains = []\n",
        "    subunits = []\n",
        "    try:\n",
        "        for polymer in data['rcsb_entry_container_identifiers']['polymer_entity_ids']:\n",
        "            polymer_url = f\"https://data.rcsb.org/rest/v1/core/polymer_entity/{pdb_id}/{polymer}\"\n",
        "            polymer_response = requests.get(polymer_url)\n",
        "            polymer_response.raise_for_status()\n",
        "            polymer_data = polymer_response.json()\n",
        "            chains.extend(polymer_data['rcsb_polymer_entity_container_identifiers']['auth_asym_ids'])\n",
        "            subunits.append(str(polymer))\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching polymer entity details for PDB ID {pdb_id}: {e}\")\n",
        "        return None\n",
        "    except ValueError:\n",
        "        print(f\"Error decoding JSON response for polymer entity of PDB ID {pdb_id}\")\n",
        "        return None\n",
        "\n",
        "    if identifier in chains:\n",
        "        for polymer in data['rcsb_entry_container_identifiers']['polymer_entity_ids']:\n",
        "            uniprot_id = get_uniprot_id(pdb_id, polymer, chain_id=identifier)\n",
        "            if uniprot_id:\n",
        "                sequence = get_sequence_from_uniprot(uniprot_id)\n",
        "                if sequence:\n",
        "                    return sequence\n",
        "            sequence = get_sequence_from_pdb(pdb_id, polymer)\n",
        "            if sequence:\n",
        "                return sequence\n",
        "\n",
        "    elif identifier in subunits:\n",
        "        for polymer in data['rcsb_entry_container_identifiers']['polymer_entity_ids']:\n",
        "            if identifier == str(polymer):\n",
        "                uniprot_id = get_uniprot_id(pdb_id, polymer)\n",
        "                if uniprot_id:\n",
        "                    sequence = get_sequence_from_uniprot(uniprot_id)\n",
        "                    if sequence:\n",
        "                        return sequence\n",
        "                sequence = get_sequence_from_pdb(pdb_id, polymer)\n",
        "                if sequence:\n",
        "                    return sequence\n",
        "    else:\n",
        "        print(f\"Identifier {identifier} not found in chains or subunits.\")\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "7oqDtT4jCI6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get full sequences\n",
        "for index, row in tqdm(df_filtered.iterrows(), total=df_filtered.shape[0]):\n",
        "    pdb_id = row['pdb_id']\n",
        "    identifier = row['cath_id'][4]\n",
        "    sequence = get_sequence(pdb_id, identifier)\n",
        "    df_filtered.at[index, 'full_sequence'] = sequence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "m2AR7tGqXfhc",
        "outputId": "8726b3f0-f01b-4ce9-8f9a-acd0559bfff9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  5%|▌         | 141/2695 [05:49<1:41:07,  2.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error extracting UniProt ID from response for PDB ID 3hpa, polymer 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  6%|▋         | 174/2695 [07:08<1:36:19,  2.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error extracting UniProt ID from response for PDB ID 3fgx, polymer 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  7%|▋         | 185/2695 [07:34<1:36:12,  2.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error extracting UniProt ID from response for PDB ID 3kwl, polymer 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 19%|█▊        | 502/2695 [20:48<1:24:17,  2.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error extracting UniProt ID from response for PDB ID 2o55, polymer 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 19%|█▉        | 518/2695 [21:25<1:31:42,  2.53s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error fetching sequence for UniProt ID 70834870: 400 Client Error: Bad Request for url: https://rest.uniprot.org/uniprotkb/70834870.fasta\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 19%|█▉        | 521/2695 [21:33<1:29:11,  2.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error extracting UniProt ID from response for PDB ID 2o57, polymer 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 22%|██▏       | 590/2695 [24:19<1:24:28,  2.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error extracting UniProt ID from response for PDB ID 3dip, polymer 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 22%|██▏       | 601/2695 [24:45<1:23:23,  2.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error extracting UniProt ID from response for PDB ID 5vis, polymer 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25%|██▍       | 670/2695 [27:35<1:32:22,  2.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error extracting UniProt ID from response for PDB ID 2r3s, polymer 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 26%|██▌       | 699/2695 [28:45<1:18:08,  2.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error extracting UniProt ID from response for PDB ID 3m6j, polymer 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 30%|██▉       | 802/2695 [32:53<1:13:31,  2.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error extracting UniProt ID from response for PDB ID 3eo6, polymer 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 33%|███▎      | 882/2695 [36:14<1:14:06,  2.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error extracting UniProt ID from response for PDB ID 3vjf, polymer 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 34%|███▎      | 905/2695 [37:09<1:14:36,  2.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error extracting UniProt ID from response for PDB ID 3kwl, polymer 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 36%|███▋      | 978/2695 [40:05<1:06:30,  2.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error extracting UniProt ID from response for PDB ID 4eog, polymer 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 37%|███▋      | 990/2695 [40:35<1:09:40,  2.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error extracting UniProt ID from response for PDB ID 2wb7, polymer 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 38%|███▊      | 1023/2695 [41:55<1:05:07,  2.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error extracting UniProt ID from response for PDB ID 2o5n, polymer 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 45%|████▌     | 1214/2695 [50:05<1:00:11,  2.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error extracting UniProt ID from response for PDB ID 3lye, polymer 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|████▉     | 1345/2695 [55:26<51:47,  2.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error extracting UniProt ID from response for PDB ID 3e0z, polymer 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 59%|█████▉    | 1585/2695 [1:05:09<43:22,  2.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error extracting UniProt ID from response for PDB ID 3kwl, polymer 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 67%|██████▋   | 1808/2695 [1:14:40<34:36,  2.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error extracting UniProt ID from response for PDB ID 3jrt, polymer 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 72%|███████▏  | 1934/2695 [1:19:44<29:24,  2.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error extracting UniProt ID from response for PDB ID 3ako, polymer 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 86%|████████▌ | 2316/2695 [1:35:33<16:06,  2.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error extracting UniProt ID from response for PDB ID 1qys, polymer 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 87%|████████▋ | 2348/2695 [1:36:50<13:33,  2.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error extracting UniProt ID from response for PDB ID 2j7q, polymer 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 93%|█████████▎| 2503/2695 [1:43:06<07:55,  2.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error extracting UniProt ID from response for PDB ID 3s9x, polymer 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 95%|█████████▌| 2563/2695 [1:45:38<05:18,  2.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error extracting UniProt ID from response for PDB ID 3qvq, polymer 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████▉| 2682/2695 [1:50:33<00:33,  2.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Identifier A not found in chains or subunits.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2695/2695 [1:51:04<00:00,  2.47s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#get real substring from full sequence based on cath indices and use this as sequence instead\n",
        "\n",
        "def extract_substring(row):\n",
        "    start, end = row['cath_indices']\n",
        "    return row['full_sequence'][start-1:end]"
      ],
      "metadata": {
        "id": "PdaA9bmmmQUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training Sequence-Based Model**"
      ],
      "metadata": {
        "id": "KdFySiJeh5GA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load in sequence embeddings\n",
        "train_embeddings_path = '/content/drive/MyDrive/Challenge (Rishab)/ml_hands_on_challenge/cath_proteins_embeddings.pkl'\n",
        "\n",
        "with open(train_embeddings_path, 'rb') as file:\n",
        "    sequence_embeddings = pickle.load(file)"
      ],
      "metadata": {
        "id": "lMkXzVebPJmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_encoder = LabelEncoder()\n",
        "\n",
        "#fit encoder on the architecture labels\n",
        "df['encoded_architecture'] = label_encoder.fit_transform(df['architecture'])"
      ],
      "metadata": {
        "id": "8wAnrS1Cijhw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dataset set up\n",
        "\n",
        "class SeqDataset(torch.utils.data.Dataset):\n",
        "   def __init__(self, df, sequence_embeddings):\n",
        "    super().__init__()\n",
        "    self.sequence_embeddings = sequence_embeddings\n",
        "    self.df = df\n",
        "\n",
        "   def __len__(self):\n",
        "    return len(self.df)\n",
        "\n",
        "   def __getitem__(self, index):\n",
        "    prot, architecture = self.df.loc[index][['sequences', 'encoded_architecture']]\n",
        "    prot_embedding = self.sequence_embeddings[prot]\n",
        "    architecture = torch.tensor(architecture, dtype=torch.long)\n",
        "\n",
        "\n",
        "    return_dict = {\n",
        "        \"Protein\": prot,\n",
        "        \"Sequence Input\": prot_embedding,\n",
        "        \"Architecture\": architecture\n",
        "    }\n",
        "\n",
        "    return return_dict"
      ],
      "metadata": {
        "id": "YHh-_HDUiDOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#self-attention sequence-based model\n",
        "\n",
        "class SeqClassifier(nn.Module):\n",
        "    def __init__(self, num_classes, embedding_dim=1280):\n",
        "        super(SeqClassifier, self).__init__()\n",
        "        #self-attention layer\n",
        "        self.attention = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=4, batch_first=True)\n",
        "\n",
        "        #fully connected layers\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(embedding_dim * 2, 1280),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1280, 640),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(640, 320),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(320, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, protein_embedding):\n",
        "        #reshape protein embedding\n",
        "        protein_embedding = protein_embedding.unsqueeze(1)\n",
        "\n",
        "        #apply self-attention\n",
        "        attention_output, _ = self.attention(protein_embedding, protein_embedding, protein_embedding)\n",
        "\n",
        "        #concat original + attention output\n",
        "        combined = torch.cat((protein_embedding.squeeze(1), attention_output.squeeze(1)), dim=1)\n",
        "\n",
        "        #pass through layers\n",
        "        output = self.fc_layers(combined)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "QSQJ00ooiK4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#setup\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import torch.optim as optim\n",
        "\n",
        "dataset = SeqDataset(df, sequence_embeddings)\n",
        "\n",
        "#match train and val sequences from earlier clustering\n",
        "train_df = df[df['sequences'].isin(train_sequences)]\n",
        "val_df = df[df['sequences'].isin(test_sequences)]\n",
        "\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "val_df = val_df.reset_index(drop=True)\n",
        "\n",
        "\n",
        "train_dataset = SeqDataset(train_df, sequence_embeddings)\n",
        "val_dataset = SeqDataset(val_df, sequence_embeddings)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "\n",
        "#model, loss function, and optimizer\n",
        "seq_model = SeqClassifier(num_classes=10)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "seq_model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(seq_model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "ypSLNmuJisar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train and eval loop\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "num_epochs = 10\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    #train\n",
        "    seq_model.train()\n",
        "    train_loss = 0.0\n",
        "    for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{num_epochs}\"):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        protein_inputs = batch[\"Sequence Input\"].to(device)\n",
        "        labels = batch[\"Architecture\"].to(device)\n",
        "\n",
        "        outputs = seq_model(protein_inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "    train_losses.append(train_loss)\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {train_loss:.4f}')\n",
        "\n",
        "    #val\n",
        "    seq_model.eval()\n",
        "    val_predictions = []\n",
        "    val_labels = []\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(val_loader, desc=f\"Validation Epoch {epoch+1}/{num_epochs}\"):\n",
        "            protein_inputs = batch[\"Sequence Input\"].to(device)\n",
        "            labels = batch[\"Architecture\"].to(device)\n",
        "\n",
        "            outputs = seq_model(protein_inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            val_predictions.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
        "            val_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    val_losses.append(val_loss)\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Validation Loss: {val_loss:.4f}')\n",
        "\n",
        "    #precision, recall, f1, and accuracy\n",
        "    val_predictions = np.array(val_predictions)\n",
        "    val_labels = np.array(val_labels)\n",
        "\n",
        "    precision = precision_score(val_labels, val_predictions, average='macro')\n",
        "    recall = recall_score(val_labels, val_predictions, average='macro')\n",
        "    f1 = f1_score(val_labels, val_predictions, average='macro')\n",
        "    accuracy = accuracy_score(val_labels, val_predictions)\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}, Accuracy: {accuracy:.4f}')\n",
        "\n",
        "    #save the best model\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(seq_model.state_dict(), '/content/drive/MyDrive/Challenge (Rishab)/ml_hands_on_challenge/seq.pt')\n",
        "        print(f'Model saved at epoch {epoch+1} with validation loss {val_loss:.4f}')\n",
        "\n",
        "#plot training and validation loss\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(1, num_epochs + 1), train_losses, label='Training Loss')\n",
        "plt.plot(range(1, num_epochs + 1), val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss Over Epochs')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5PHrxZqfjBuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training Seq+Struct Model**"
      ],
      "metadata": {
        "id": "Byl8k1mOjXq2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "structure_embeddings_path = '/content/drive/MyDrive/Challenge (Rishab)/ml_hands_on_challenge/structure_embeddings.pkl'\n",
        "\n",
        "with open(structure_embeddings_path, 'rb') as file:\n",
        "    structure_embeddings = pickle.load(file)"
      ],
      "metadata": {
        "id": "u3qiqa_rwXiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dataset class setup\n",
        "\n",
        "class SeqStructDataset(torch.utils.data.Dataset):\n",
        "   def __init__(self, df, sequence_embeddings, structure_embeddings):\n",
        "    super().__init__()\n",
        "    self.sequence_embeddings = sequence_embeddings\n",
        "    self.structure_embeddings = structure_embeddings\n",
        "    self.df = df\n",
        "\n",
        "   def __len__(self):\n",
        "    return len(self.df)\n",
        "\n",
        "   def __getitem__(self, index):\n",
        "    prot, architecture = self.df.loc[index][['sequences', 'encoded_architecture']]\n",
        "    prot_embedding = self.sequence_embeddings[prot]\n",
        "    structure_embedding = self.structure_embeddings[prot].squeeze(0)\n",
        "    architecture = torch.tensor(architecture, dtype=torch.long)\n",
        "\n",
        "\n",
        "    return_dict = {\n",
        "        \"Protein\": prot,\n",
        "        \"Sequence Input\": prot_embedding,\n",
        "        \"Structure Input\": structure_embedding,\n",
        "        \"Architecture\": architecture\n",
        "    }\n",
        "\n",
        "    return return_dict"
      ],
      "metadata": {
        "id": "ZiZRMcr_iJ41"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sequence + structure attention model\n",
        "\n",
        "class SeqStructureClassifier(nn.Module):\n",
        "    def __init__(self, num_classes, sequence_embedding_dim=1280, structure_embedding_dim=640):\n",
        "        super(SeqStructureClassifier, self).__init__()\n",
        "\n",
        "        #self-attention layers for sequence and structure\n",
        "        self.sequence_attention = nn.MultiheadAttention(embed_dim=sequence_embedding_dim, num_heads=4, batch_first=True)\n",
        "        self.structure_attention = nn.MultiheadAttention(embed_dim=structure_embedding_dim, num_heads=4, batch_first=True)\n",
        "\n",
        "        #fully connected layers\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear((sequence_embedding_dim + structure_embedding_dim) * 2, 1280),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1280, 640),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(640, 320),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(320, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, sequence_embedding, structure_embedding):\n",
        "\n",
        "        #reshape embeddings\n",
        "        sequence_embedding = sequence_embedding.unsqueeze(1)\n",
        "        structure_embedding = structure_embedding.unsqueeze(1)\n",
        "\n",
        "        #apply self-attention to sequence and structure embeddings\n",
        "        sequence_attention_output, _ = self.sequence_attention(sequence_embedding, sequence_embedding, sequence_embedding)\n",
        "        structure_attention_output, _ = self.structure_attention(structure_embedding, structure_embedding, structure_embedding)\n",
        "\n",
        "        #concat original + attention embeddings\n",
        "        sequence_combined = torch.cat((sequence_embedding.squeeze(1), sequence_attention_output.squeeze(1)), dim=1)\n",
        "        structure_combined = torch.cat((structure_embedding.squeeze(1), structure_attention_output.squeeze(1)), dim=1)\n",
        "\n",
        "        #concatenate sequence and structure embeddings\n",
        "        combined = torch.cat((sequence_combined, structure_combined), dim=1)\n",
        "\n",
        "        #pass through fc layers\n",
        "        output = self.fc_layers(combined)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "iJrgmQRMxOJw"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#setup\n",
        "\n",
        "#match train and val sequences from earlier clustering\n",
        "train_df = df[df['sequences'].isin(train_sequences)]\n",
        "val_df = df[df['sequences'].isin(test_sequences)]\n",
        "\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "val_df = val_df.reset_index(drop=True)\n",
        "\n",
        "train_dataset = SeqStructDataset(train_df, sequence_embeddings, structure_embeddings)\n",
        "val_dataset = SeqStructDataset(val_df, sequence_embeddings, structure_embeddings)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "\n",
        "#model, loss function, and optimizer\n",
        "combined_model = SeqStructureClassifier(num_classes=10)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "combined_model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(combined_model.parameters(), lr=0.0001)"
      ],
      "metadata": {
        "id": "SlcIjjNBTwp2"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train + eval\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "num_epochs = 10\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    #train\n",
        "    combined_model.train()\n",
        "    train_loss = 0.0\n",
        "    for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{num_epochs}\"):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        sequence_inputs = batch[\"Sequence Input\"].to(device)\n",
        "        structure_inputs = batch[\"Structure Input\"].to(device)\n",
        "        labels = batch[\"Architecture\"].to(device)\n",
        "\n",
        "        outputs = combined_model(sequence_inputs, structure_inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "    train_losses.append(train_loss)\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {train_loss:.4f}')\n",
        "\n",
        "    #val\n",
        "    combined_model.eval()\n",
        "    val_predictions = []\n",
        "    val_labels = []\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(val_loader, desc=f\"Validation Epoch {epoch+1}/{num_epochs}\"):\n",
        "            sequence_inputs = batch[\"Sequence Input\"].to(device)\n",
        "            structure_inputs = batch[\"Structure Input\"].to(device)\n",
        "            labels = batch[\"Architecture\"].to(device)\n",
        "\n",
        "            outputs = combined_model(sequence_inputs, structure_inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            val_predictions.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
        "            val_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    val_losses.append(val_loss)\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Validation Loss: {val_loss:.4f}')\n",
        "\n",
        "    #precision, recall, f1, and accuracy\n",
        "    val_predictions = np.array(val_predictions)\n",
        "    val_labels = np.array(val_labels)\n",
        "\n",
        "    precision = precision_score(val_labels, val_predictions, average='macro')\n",
        "    recall = recall_score(val_labels, val_predictions, average='macro')\n",
        "    f1 = f1_score(val_labels, val_predictions, average='macro')\n",
        "    accuracy = accuracy_score(val_labels, val_predictions)\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}, Accuracy: {accuracy:.4f}')\n",
        "\n",
        "    #save the best model\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(combined_model.state_dict(), '/content/drive/MyDrive/Challenge (Rishab)/ml_hands_on_challenge/seq_struct.pt')\n",
        "        print(f'Model saved at epoch {epoch+1} with validation loss {val_loss:.4f}')\n",
        "\n",
        "#plot training and validation loss\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(1, num_epochs + 1), train_losses, label='Training Loss')\n",
        "plt.plot(range(1, num_epochs + 1), val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss Over Epochs')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "xH5mmvOsyEPk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}